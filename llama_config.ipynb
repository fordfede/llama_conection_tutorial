{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfaLp0EAFxDF3S7CE17wel"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instalación"
      ],
      "metadata": {
        "id": "fWMHEnmxYrZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependencias (ignorar warning)\n",
        "\n",
        "!pip install langchain langchain-community transformers torch accelerate"
      ],
      "metadata": {
        "id": "HRgDSUkF-U6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuración"
      ],
      "metadata": {
        "id": "2O-7QCxWYu4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar Llama desde Hugging Face (ignorar warning)\n",
        "\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch"
      ],
      "metadata": {
        "id": "x95Y8tcC-Z0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "login(token=userdata.get('colab-token'))"
      ],
      "metadata": {
        "id": "_lMVAX0TAMid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxTb1J_G95LU"
      },
      "outputs": [],
      "source": [
        "# Configuracón del modelo\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map =\"auto\",\n",
        "    dtype = torch.float16\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens= 300,\n",
        "    temperature=0.1,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test del modelo\n",
        "\n",
        "test = llm.invoke(\"¿Cuál es la capital de Argentina?\")\n",
        "print(test)"
      ],
      "metadata": {
        "id": "0NvRHwNKpSMj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}