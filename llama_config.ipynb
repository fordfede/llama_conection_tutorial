{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependencias (ignorar warning)\n",
        "\n",
        "!pip install langchain langchain-community transformers torch accelerate"
      ],
      "metadata": {
        "id": "HRgDSUkF-U6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar Llama desde Hugging Face (ignorar warning)\n",
        "\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ],
      "metadata": {
        "id": "x95Y8tcC-Z0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "login(token=userdata.get('colab-token'))"
      ],
      "metadata": {
        "id": "_lMVAX0TAMid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxTb1J_G95LU"
      },
      "outputs": [],
      "source": [
        "# Configuracón del modelo\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=50,\n",
        "    temperature=0.1,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test del modelo\n",
        "\n",
        "test = llm.invoke(\"¿Cuál es la capital de Argentina?\")\n",
        "print(test)"
      ],
      "metadata": {
        "id": "zPtwDDQXD0_H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}